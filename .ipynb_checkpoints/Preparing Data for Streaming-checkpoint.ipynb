{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing our Network Data For Streaming\n",
    "--------------------------------------\n",
    "\n",
    "Recall from our introduction, that we are going to use netflow data from Los Alamos National Laboratory. It's a huge dataset and comes to us in CSV format. To better simulate true streaming, we're going to convert it to JSON; to allow us to run it locally, we'll only use a small subset of the data.\n",
    "\n",
    "If we wanted to replicate this exercise on a larger dataset, we could set up a larger Spark cluster on a cloud provider, or in a datacenter. We could also add a Kafka / Zookeeper layer to our environment as well. Both of those steps would take us closer to a more realistic production environment.\n",
    "\n",
    "For this part of the exercise, we'll:\n",
    "    1. Get a network file from LANL\n",
    "    2. Open the first 500,000 rows - which is only about 2.5 minutes of data - in Pandas\n",
    "    3. Write the partial dataset as a single CSV.\n",
    "    4. Split the CSV into multiple 10,000 row CSVs (50 total).\n",
    "    5. Convert the 50 CSV's into streaming formatted JSON.\n",
    "    \n",
    "You're free to adjust the total number of rows, as well as the size of the JSONs, but we find that this provides a good balance of data volume and timely processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two choices to work with the data here - since the data directory should be mapped as a volume into your Docker container, you can either access the container itself using `docker exec pyspark1 /bin/bash` (if your container is named `pyspark1`. Alternatively, you can use the mapped volume locally on your system.\n",
    "\n",
    "Either way, `cd` into the directory where you want this data to live so that all our data is centrally located.\n",
    "\n",
    "Then, get the data. This example grabs day 3. Use [this link to download the dataset](https://s3-us-gov-west-1.amazonaws.com/unified-host-network-dataset/2017/netflow/netflow_day-03.bz2), and then move it into your `big-data-student-resources/datasets/lanl` directory.\n",
    "\n",
    "Once that finishes downloading, you can come back to this notebook and run the Python steps here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import Pandas, define a headers list, and then read the CSV.\n",
    "\n",
    "Pandas is great for this step because it natively decompresses the bz2 format, can apply headers when there aren't any, and can easily limit the number of rows we import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = ['time', 'duration', 'srcdevice', 'dstdevice', 'protocol',\n",
    "           'srcport', 'dstport', 'srcpackets', 'dstpackets', 'srcbytes', 'dstbytes']\n",
    "\n",
    "dfDay03 = pd.read_csv('./datasets/lanl/netflow_day-03.bz2', header=None, names=headers, nrows=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check the basics of our imported dataframe for length and to make sure the data looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfDay03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>duration</th>\n",
       "      <th>srcdevice</th>\n",
       "      <th>dstdevice</th>\n",
       "      <th>protocol</th>\n",
       "      <th>srcport</th>\n",
       "      <th>dstport</th>\n",
       "      <th>srcpackets</th>\n",
       "      <th>dstpackets</th>\n",
       "      <th>srcbytes</th>\n",
       "      <th>dstbytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>172957</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp208915</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port27607</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>172957</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp208915</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port55333</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>172957</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp208915</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port74721</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>172957</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp208915</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port80500</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>172957</td>\n",
       "      <td>0</td>\n",
       "      <td>Comp208915</td>\n",
       "      <td>Comp275646</td>\n",
       "      <td>17</td>\n",
       "      <td>Port81522</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          time  duration   srcdevice   dstdevice  protocol    srcport dstport  \\\n",
       "499995  172957         0  Comp208915  Comp275646        17  Port27607      53   \n",
       "499996  172957         0  Comp208915  Comp275646        17  Port55333      53   \n",
       "499997  172957         0  Comp208915  Comp275646        17  Port74721      53   \n",
       "499998  172957         0  Comp208915  Comp275646        17  Port80500      53   \n",
       "499999  172957         0  Comp208915  Comp275646        17  Port81522      53   \n",
       "\n",
       "        srcpackets  dstpackets  srcbytes  dstbytes  \n",
       "499995           1           0        73         0  \n",
       "499996           1           0        60         0  \n",
       "499997           1           0        65         0  \n",
       "499998           1           0        63         0  \n",
       "499999           1           0        63         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDay03.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all we need from Pandas here is a smaller CSV, go ahead and write it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDay03.to_csv('./datasets/lanl/netflow_day-03_partial.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll go back to the command line and process the csv.\n",
    "\n",
    "Note that these commands were developed using `zsh` on macOS. If you are using `bash` or another shell, the syntax will likely be slightly different.\n",
    "\n",
    "First, let's split the large CSV into the smaller files. From the terminal, type:\n",
    "`split -l 10000 netflow_day-03_partial.csv segment_`\n",
    "\n",
    "You'll now have 50 new files names `segment_aa` through `segment_bx`. It's time to convert them to JSON.\n",
    "\n",
    "We'll do this using the `csvkit` utility. If you haven't installed it, you can do so via a simple `pip install csvkit`. Within `csvkit`, there is a command called `csvjson` that converts CSV files into JSON.\n",
    "\n",
    "First, make a single headers file that will be used on each of the smaller splits:\n",
    "`head -1 netflow_day-03_partial.csv > headers.csv`\n",
    "\n",
    "Now, create a shell script in your preferred text editor with the following content (again, written in `zsh`):\n",
    "\n",
    "```\n",
    "#!/bin/zsh\n",
    "set -e\n",
    "\n",
    "fnumber=1\n",
    "\n",
    "files=(segment_*(N))\n",
    "\n",
    "for input in $files; do\n",
    "    printf -v j \"%02g\" $fnumber\n",
    "    echo \"working on time_$j.json\"\n",
    "    cat headers.csv $input > time_$j.csv\n",
    "    csvjson --stream time_$j.csv > time_$j.json\n",
    "    rm time_$j.csv $input\n",
    "    fnumber=$((fnumber + 1))\n",
    "done\n",
    "```\n",
    "\n",
    "Save the file, and from the command line make sure you `chmod` it to be executable (`chmod a+x <your_script>.sh`).\n",
    "\n",
    "All this script does is create a list of all the `segment_` files, then iterate over each one to create a JSON file from the CSV. When this is complete, your 50 `segment_` files will have been replaced by `time_xx.json` files, where `xx` is a value between `01` and `50`.\n",
    "\n",
    "At this point, you should move the original CSV, headers.csv, the downloaded bz2, and the shell script file out of the directory so that there are only JSON files remaining. This is important for Spark to handle streaming from a directory.\n",
    "\n",
    "Congratulations! You've successfully prepared our dataset for streaming. Time to dive into the fun part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
