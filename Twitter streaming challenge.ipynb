{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Your Queries In Spark\n",
    "---------------------------\n",
    "\n",
    "You need to take the data from Twitter and perform your analysis based on the question you pose above.\n",
    "\n",
    "In our hashtag counting example, we do the following:\n",
    "1. Explode the tweet into its individual words.\n",
    "2. Filter out hashtags, then each tag gets assigned the count 1 with the `map` method.\n",
    "3. Append the current hashtags to the main dataframe and update the counts appropriately.\n",
    "4. Query the temporary table to return the current top 10 hashtags trending, and show to the console.\n",
    "\n",
    "You can extend this into a web dashboard, or plots inside this notebook if you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext, SparkSession\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some constants, initialize Spark, and then open the socket with the remote host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCP_REMOTE_HOST = \"data_server\"\n",
    "TCP_PORT = 9009\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(TCP_REMOTE_HOST, TCP_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now create some helper functions to allow Spark to maintain our running count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(f\"----------- {str(time)} -----------\")\n",
    "    try:\n",
    "        # Get spark sql singleton context from the current context\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        \n",
    "        # convert the RDD to Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        \n",
    "        # create a DF from the Row RDD\n",
    "        hashtags_df = spark.createDataFrame(row_rdd)\n",
    "        \n",
    "        # Register the dataframe as table\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        #hashtags_df.createOrReplaceTempView(\"hashtags\")\n",
    "        \n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        hashtag_counts_df = spark.sql(\n",
    "            \"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        hashtag_counts_df.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        e = sys.exc_info()[1]\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we assign our primary workflow that will utilize the above functions.\n",
    "\n",
    "After that's complete, we begin the streaming with `ssc.start()`. The query stays open until we terminate it (`ssc.awaitTermination()`).\n",
    "\n",
    "We can end the streaming by going to the `data_server` container and typing `ctrl-C`. Recall that you can exit the container shell with the `Ctrl-P, Ctrl-Q` sequence.\n",
    "\n",
    "If you are using tweets from a geofenced area (as we are here) - you should let it run for a while in order to build up enough data to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
