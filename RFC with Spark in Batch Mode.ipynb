{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Random Forest Classifier with Spark in Batch Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to get into Apache Spark, and the best way to do so is by experimenting with some code.\n",
    "\n",
    "We'll start by going over some terms, explain why we chose the dataset we'll use, and finally, build a classifier to identify movement type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How will we use Spark in this example?\n",
    "\n",
    "For this example, we'll use a local, standalone Apache Spark. In fact, we're going to use Spark in this way through most of the examples and exercises. By starting with a simple deployment, we can focus on the details of developing machine learning solutions with Spark and postpone the complexities of deploying a cluster until after we understand the details of implementing machine learning solutions with Spark.\n",
    "\n",
    "We'll begin our machine learning with Spark exercise by using it in __batch mode__. This mode is most similar to the approach you've taken so far in this course - load a dataset, clean it, then run models and evaluate their performance.\n",
    "\n",
    "__Streaming mode__ differs from batch mode in that, under streaming mode, you do not have access to all of the data. Instead, you ingest data into Spark over a period of time. As each data element arrives, you perform data science tasks (clean, classify, etc) on the new data, then archive the result. Don't worry about the details of this now - we'll cover it soon enough. For now, you only need to be aware that there are two modes we can work from in Spark.\n",
    "\n",
    "Once you're confident you can use local, standalone Spark in both batch and streaming mode, we'll work through how to create, deploy, and manage a Spark cluster in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Imports:\n",
    "\n",
    "Spark works a little different than the data science stack you've worked with already. In the data science stack, you are writing Python code that both executes immediately and also executes locally on the device you're using.\n",
    "\n",
    "Spark, on the other hand, lives outside the Python environment. We treat it as a server that we pass instructions to; the Spark server performs the calculations and then reports back to our notebook. Additionally, Spark uses the concept of a __pipeline__ in which we configure a number of steps, that are only executed when we tell the server to run.\n",
    "\n",
    "There are a number of benefits to this approach for big data. First of all, by using the server paradigm, we can run locally with a smaller set of data when we're building and testing code. Then, when we've successfully set up our model, deploying to production means we point our Python code to a remote server that is configured to handle larger datasets. Second, the pipeline approach allows us to configure models and run them through multiple steps more efficiently on big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these imports allow us to set up our Python connection to the Spark server.\n",
    "# it also allows us to load a dataframe.\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# these imports are how we build and manager our data science processes: cleaning data, preparing a model,\n",
    "# executing the model, and evaluating the model.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to review the documentation for modules, methods, and functions we'll use. Everything we look at here can be found in the Apache Spark documentation.\n",
    "\n",
    "In particular, you should familiarize yourself with the:\n",
    "* [Spark Overview](https://spark.apache.org/docs/2.1.0/index.html)\n",
    "* [Python API Documentation](https://spark.apache.org/docs/2.1.0/api/python/index.html)\n",
    "\n",
    "For our work here, it's also good to read about random forests in Spark [here](https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#random-forests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a set of constants for clarity and simplicity in managing the notebook.\n",
    "# this allows you to refer back to this cell at any time if you need to either confirm or modify any of these values.\n",
    "\n",
    "CSV_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/allData.csv\"\n",
    "CSV_ACTIVITY_LABEL_PATH = \"/home/ds/notebooks/datasets/UCI_HAR/activity_labels.csv\"\n",
    "APP_NAME = \"UCI HAR Random Forest Example\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 141107\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RF_NUM_TREES = 10\n",
    "RF_MAX_DEPTH = 4\n",
    "RF_NUM_BINS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started:\n",
    "\n",
    "1. Create the connection to the Spark server using the SparkSession.\n",
    "2. After that's done, import two datasets and do some cleaning and validation before we configure our model.\n",
    "\n",
    "We import two datasets:\n",
    "    1. `activity_labels`: a mapping of the classifier labels used in the datasets.\n",
    "    2. `df`: a generic name for the dataset we're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASIDE: THE UCI HAR Dataset\n",
    "\n",
    "We'll be using a slightly modified version of the UCI HAR dataset. This is a particularly nice dataset to use when learning Spark since everything is already numeric, and there is minimal cleaning needed.\n",
    "\n",
    "This dataset contains data from experiments on 30 subjects who wore a smartphone attached to their waist. They performed six activities:\n",
    "* WALKING\n",
    "* WALKING UP STAIRS\n",
    "* WALKING DOWN STAIRS\n",
    "* SITTING\n",
    "* STANDING\n",
    "* LAYING\n",
    "\n",
    "The activities were recorded with video, which enabled time-stamped correlation with motion values collected from the smartphones. In particular, 3-axial linear acceleration and 3-axial angular velocity were captured from the device's accelerometer and gyroscope. This resulted in a 561-value feature vector, plus its corresponding activity label.\n",
    "\n",
    "The source dataset is available [here](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones). \n",
    "\n",
    "If you download this, you'll see that there are actually four datasets:\n",
    "1. __Train__\n",
    "    * X_features\n",
    "    * Y_labels\n",
    "2. __Test__\n",
    "    * X_features\n",
    "    * Y_labels\n",
    "    \n",
    "To use this in Spark, we do a little bit of preparation outside this example. We'll describe the steps, in the event you choose to try to prep the data yourself (it's always a good exercise to flex your data munging muscles when you can).\n",
    "\n",
    "Here's what we did to generate the `allData.csv` file:\n",
    "1. The source files as provided are space-delimited. And, unfortunately, the files are inconsistent with the spacing. Most of the time there are only single space delimiters, but you find double spaces intermittently too. Those double spaces add extraneous columns to the dataset and create problems for our classifier.\n",
    "In order to get around this, we replaced the spaces with commas. It's not necessary but mainly for personal preferences. We also do a string replace to get rid of all double-delimiters and replace with single delimiters. \n",
    "When you've completed this step, you can import and should see that you have __561__ unique feature columns in both the train and test datasets.\n",
    "\n",
    "2. We also merge the features and labels datasets, then append the test dataset onto the train dataset.\n",
    "\n",
    "The final dataset should have __10,299__ rows and __562__ columns. Everything should be numeric - the labels will be integers and the features will be doubles. Because the source data is already numeric, it's simpler to build and demonstrate our classifier in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why didn't we use the Lending Club dataset from our earlier random forest exercise?\n",
    "\n",
    "Great question! The simple answer is that we could have used that dataset for this exercise. However, Spark explicitly demands that all inputs to the random forest classifier be indexed labels and indexed numeric feature vectors. The Lending Club data doesn't easily translate from the delivered format into Spark-ready formats. \n",
    "\n",
    "Thus, rather than spend 90% of an exercise on data cleaning and preparation, we'll use the UCI HAR dataset. It requires only a minimal amount of preparation and cleaning, so we can spend most of our time on the relevant Spark material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "\n",
    "activity_labels = spark.read.options(inferschema = \"true\").csv(CSV_ACTIVITY_LABEL_PATH)\n",
    "\n",
    "df = spark.read.options(inferschema = \"true\").csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation\n",
    "\n",
    "As mentioned above, if we cleaned and prepared the data properly, the dataset will meet the following three conditions.\n",
    "\n",
    "Look over the datasets for:\n",
    "1. Final shape should be 10299 rows by 562 columns\n",
    "2. All feature columns should be doubles\n",
    "3. There should be no nulls\n",
    "    * This is important since Spark will fail to build our vector variables that we need in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we get going, we first create some diagnostic variables to conduct our tests.\n",
    "\n",
    "# Testing for data types\n",
    "# Use a list comprehension to grab the column names that all have the data type 'double'\n",
    "double_cols = [col[0] for col in df.dtypes if col[1] == 'double']\n",
    "\n",
    "# Testing for nulls in columns. \n",
    "# We use the dataframe select method to build a list that is then converted to a Python dict.\n",
    "# This way it's easy to sum up the nulls in a moment when we're actually testing for presence of nulls. \n",
    "null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "                         for c in df.columns]).toPandas().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, since we're using a notebook, we'll just pretty things up by using `print` statements to confirm the tests we mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is 10299 rows by 562 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561 columns out of 562 total are type double.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(double_cols):d} columns out of {len(df.columns):d} total are type double.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {sum(null_counts[0].values()):d} null values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up and running a classifier in Spark\n",
    "\n",
    "Assuming that the data is now clean, we are now ready to reshape the data and run the random forest model.\n",
    "\n",
    "In Spark, we manipulate the data to work in a Spark pipeline, define each of the steps in the pipeline, chain them together in a pipeline, and finally run the pipeline.\n",
    "\n",
    "The Apache Spark classifiers expect two columns of input:\n",
    "1. __labels__: an indexed set of numeric variables that represent the classification from the set of features we provide.\n",
    "2. __features__: An indexed, vector variable that contains all of the feature values in each row. \n",
    "\n",
    "In order to do this, we need to create these two columns from our dataset - the data is there, but not yet in a format we can use in the classifier.\n",
    "\n",
    "To create the indexed labels column, we create a column called `indexedLabel` using the `StringIndexer` method.\n",
    "1. We use the column `_c0` as the source for our label index since that contains our labels. The column contains only one value per index.\n",
    "    \n",
    "To create the indexed features column, we need to do two steps:\n",
    "1. Create the vector of features using the `VectorAssembler` method.\n",
    "    * We need to use all numeric columns in our data frame - there are 561 - to create this vector.\n",
    "    * The vector assembler creates a new column called `features`; each row of this column contains a 561-element vector that is built from the 561 features in the dataset.\n",
    "\n",
    "2. Complete the data preparation by creating an indexed vector from the `features` column.\n",
    "    * This vector is called `indexedFeatures`.\n",
    "    \n",
    "Since the classifier expects indexed labels and an indexed vector column of data, we use the `indexedLabel` and `indexedFeatures` as inputs to our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our feature vector.\n",
    "# Note that we're doing the work on the `df` object - we don't create new dataframes, \n",
    "# just add columns to the one we already are using.\n",
    "\n",
    "# the transform method performs the act of creating the column.\n",
    "\n",
    "df = VectorAssembler(inputCols=double_cols, outputCol=\"features\").transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the features are there.\n",
    "\n",
    "It's easy to do this in Apache Spark using the `select` and `show` methods on the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|            features|\n",
      "+---+--------------------+\n",
      "|  5|[0.289,-0.0203,-0...|\n",
      "|  5|[0.278,-0.0164,-0...|\n",
      "|  5|[0.28,-0.0195,-0....|\n",
      "|  5|[0.279,-0.0262,-0...|\n",
      "|  5|[0.277,-0.0166,-0...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"_c0\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build the indexers, split our data for training and testing, define our model, and finally chain everything together into a pipeline.\n",
    "\n",
    "* __It is important to note - when we execute this cell, we're not actually running our model. We're only defining its parameters in this cell.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training indexers / split data / classifier\n",
    "# first we'll generate a labelIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"_c0\", outputCol=\"indexedLabel\").fit(df)\n",
    "\n",
    "# now generate the indexed feature vector.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n",
    "    \n",
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell runs the pipeline - delivering a trained model at the end of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now easy to test our model and make predictions, simply by using the model's `transform` method on the `testData` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model\n",
    "\n",
    "We can use the MulticlassClassificationEvaluator to test the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0811202\n",
      "Accuracy = 0.91888\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "\n",
    "So we've seen how to set up some data and build a classifier in Spark. You might want to play around with this notebook and learn more about how Spark works.\n",
    "\n",
    "Some ideas:\n",
    "1. Look at the set of labels, and see if there are features that might be better if combined. Spark has a means to map values into a new column.\n",
    "\n",
    "2. Identify the most important features among the 561 source features (using PCA or something similar); reduce the feature set and see if the model performs better.\n",
    "\n",
    "3. Modify the settings of the random forest to see if the performance improves.\n",
    "\n",
    "4. Use Spark's tools to find other techniques to evaluate the performance of your model - see if you can figure out how to generate a ROC plot, find the AUC value, or plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
